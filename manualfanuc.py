import streamlit as st
import openai
import time
import os
# Importa GoogleTranslator apenas uma vez
from deep_translator import GoogleTranslator
# N√£o precisa importar streamlit e GoogleTranslator novamente

# Carregar vari√°veis de ambiente do arquivo .env (ainda √∫til para a API Key)
# Certifique-se de ter python-dotenv instalado (pip install python-dotenv)
from dotenv import load_dotenv
load_dotenv()

# Inicializar cliente OpenAI
# A chave da API ser√° buscada automaticamente da vari√°vel de ambiente OPENAI_API_KEY
try:
    client = openai.Client()
except Exception as e:
    st.error(f"Erro ao inicializar cliente OpenAI. Verifique se OPENAI_API_KEY est√° no seu arquivo .env: {e}")
    st.stop() # Parar a execu√ß√£o se o cliente n√£o puder ser inicializado


# Configura√ß√£o inicial da interface
st.title("ü§ñ Tutor Virtual para FANUC")

# Inicializa√ß√£o de estados da sess√£o
# Mantemos assistant e vector_store na sess√£o para reutilizar ap√≥s a configura√ß√£o inicial
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "assistant" not in st.session_state:
    st.session_state.assistant = None
# Usamos 'configurado' para saber se a configura√ß√£o inicial pesada j√° rodou nesta sess√£o
if "configurado" not in st.session_state:
    st.session_state.configurado = False
# Novo estado para armazenar o ID da thread para manter o hist√≥rico da conversa
if "thread_id" not in st.session_state:
    st.session_state.thread_id = None
# Estado para armazenar o hist√≥rico de mensagens exibidas
if "messages" not in st.session_state:
    st.session_state.messages = []


# Fun√ß√£o para traduzir respostas para portugu√™s
def traduzir_resposta(resposta):
    """Traduz a resposta para portugu√™s, com tratamento b√°sico de erros."""
    try:
        if not resposta or len(resposta.strip()) < 5: # Evita traduzir strings vazias ou muito curtas
            return resposta
        return GoogleTranslator(source='auto', target='pt').translate(resposta)
    except Exception as e:
        st.warning(f"Erro na tradu√ß√£o: {e}")
        return resposta # Retorna a resposta original em caso de erro


# Fun√ß√£o para obter todos os arquivos PDF da pasta 'files'
def obter_arquivos_da_pasta(pasta="files"):
    """Lista todos os arquivos PDF em uma pasta espec√≠fica."""
    if not os.path.exists(pasta):
        st.error(f"Pasta '{pasta}' n√£o encontrada.")
        return []
    arquivos_pdf = [os.path.join(pasta, f) for f in os.listdir(pasta) if f.endswith(".pdf")]
    return arquivos_pdf


# Fun√ß√£o para configurar o sistema (cria Vector Store e Assistente)
# Esta fun√ß√£o ainda √© lenta na primeira execu√ß√£o ap√≥s um restart completo do script.
def configurar_sistema():
    """Configura o Vector Store e o Assistente da OpenAI se ainda n√£o estiverem configurados na sess√£o."""
    if not st.session_state.configurado:
        with st.spinner("Configurando o tutor virtual (isso pode levar alguns minutos na primeira execu√ß√£o)..."):
            try:
                arquivos_pdf = obter_arquivos_da_pasta()

                if not arquivos_pdf:
                    st.error("Nenhum arquivo PDF encontrado na pasta 'files'.")
                    return

                # --- Cria√ß√£o do Vector Store ---
                # O ideal seria buscar um Vector Store existente pelo nome ou ID
                # Mas seguindo a restri√ß√£o de n√£o usar IDs externos, criamos um novo.
                # Isso √© o principal ponto de lentid√£o na primeira execu√ß√£o.
                # st.info("Criando Vector Store...") # Removido
                vector_store = client.vector_stores.create(name="Manual_FANUC_AppSession") # Nome para identificar

                # st.info(f"Fazendo upload de {len(arquivos_pdf)} arquivo(s) para o Vector Store...") # Removido
                file_streams = [open(file_path, "rb") for file_path in arquivos_pdf]
                try:
                    file_batch = client.vector_stores.file_batches.upload_and_poll(
                        vector_store_id=vector_store.id,
                        files=file_streams
                    )
                    # st.info(f"Status do upload e processamento: {file_batch.status}") # Removido
                    if file_batch.status != 'completed':
                         st.warning(f"Alguns arquivos podem n√£o ter sido processados corretamente. Status: {file_batch.status}")
                         # Opcional: Adicionar l√≥gica para verificar quais falharam: file_batch.file_counts.failed

                finally:
                    # Fechar os streams de arquivo
                    for fs in file_streams:
                        fs.close()

                # --- Cria√ß√£o do Assistente ---
                # st.info("Criando Assistente...") # Removido
                assistant = client.beta.assistants.create(
                    name="Especialista FANUC AppSession", # Nome para identificar
                    instructions="Voc√™ √© um especialista t√©cnico em rob√≥tica industrial com amplo conhecimento nos manuais FANUC. Responda √†s perguntas de forma clara e t√©cnica, citando sempre as p√°ginas relevantes do manual e traduzindo qualquer conte√∫do necess√°rio para portugu√™s.",
                    tools=[{"type": "file_search"}],
                    tool_resources={"file_search": {"vector_store_ids": [vector_store.id]}},
                    model="gpt-4o-mini"
                )
                st.session_state.vector_store = vector_store
                st.session_state.assistant = assistant
                st.session_state.configurado = True
                st.success("Sistema configurado com sucesso!")

            except Exception as e:
                st.error(f"Erro na configura√ß√£o: {str(e)}")
                # Em caso de erro na configura√ß√£o, resetar o estado para tentar novamente?
                # Ou parar? Vamos parar para evitar loop de erro.
                st.stop()


# --- Chamando a configura√ß√£o automaticamente apenas uma vez por sess√£o ---
# Esta chamada ainda causar√° lentid√£o na primeira execu√ß√£o completa do script.
configurar_sistema()

# Exibir hist√≥rico de mensagens (opcional, para dar contexto visual)
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

# Interface de perguntas
user_question = st.text_input("üìù Fa√ßa sua pergunta sobre os manuais FANUC:")

# Processamento da pergunta
if user_question and st.session_state.assistant: # Verifica se o assistente foi configurado
    # Adicionar pergunta do usu√°rio ao hist√≥rico (opcional)
    # st.session_state.messages.append({"role": "user", "content": user_question})
    # with st.chat_message("user"):
    #     st.markdown(user_question)

    with st.spinner("Analisando os manuais e preparando resposta..."):
        try:
            # --- Reutiliza a thread ou cria uma nova se for a primeira pergunta da sess√£o ---
            if st.session_state.thread_id is None:
                 st.info("Iniciando nova conversa...")
                 thread = client.beta.threads.create()
                 st.session_state.thread_id = thread.id
                 # st.info(f"Nova Thread ID: {st.session_state.thread_id}") # Para debug
            else:
                 # Opcional: Tentar recuperar a thread existente para verificar se ainda √© v√°lida
                 try:
                     thread = client.beta.threads.retrieve(st.session_state.thread_id)
                     # st.info(f"Continuando conversa na Thread ID: {st.session_state.thread_id}") # Para debug
                 except Exception:
                      # Se a thread n√£o existir mais (ex: expirou), cria uma nova
                      st.warning("Thread anterior n√£o encontrada ou expirou. Iniciando nova conversa...")
                      thread = client.beta.threads.create()
                      st.session_state.thread_id = thread.id
                      # st.info(f"Nova Thread ID criada: {st.session_state.thread_id}") # Para debug


            # Adiciona a mensagem do usu√°rio √† thread (usa o ID da thread armazenado)
            message = client.beta.threads.messages.create(
                thread_id=st.session_state.thread_id,
                role="user",
                content=user_question
            )

            # Executa o assistente na thread (usa o ID do assistente armazenado e o ID da thread)
            run = client.beta.threads.runs.create(
                thread_id=st.session_state.thread_id,
                assistant_id=st.session_state.assistant.id,
                # instructions="Opcional: Sobrescrever instru√ß√µes do assistente para esta run espec√≠fica"
            )

            # Polling para verificar o status da execu√ß√£o
            # Reduzimos o tempo de sleep para verificar o status mais frequentemente
            while run.status in ["queued", "in_progress", "cancelling"]:
                time.sleep(0.2) # Espera menor (0.2 segundos)
                run = client.beta.threads.runs.retrieve(thread_id=st.session_state.thread_id, run_id=run.id)
                # st.write(f"Status do Run: {run.status}") # Opcional: mostrar status para debug

            if run.status == "completed":
                # Recupera as mensagens ap√≥s a conclus√£o
                # Buscamos apenas as mensagens mais recentes, limitando a 1 (a resposta do assistente)
                messages = client.beta.threads.messages.list(
                    thread_id=st.session_state.thread_id,
                    order="desc", # Obter as mensagens mais recentes primeiro
                    limit=1 # Pegar apenas a √∫ltima mensagem
                )

                # Encontrar a √∫ltima mensagem do assistente para esta run espec√≠fica
                # √â mais robusto verificar se a mensagem pertence √† run atual
                assistant_messages_for_this_run = [
                    msg for msg in messages.data
                    if msg.run_id == run.id and msg.role == "assistant"
                ]

                if assistant_messages_for_this_run:
                    # A resposta est√° no primeiro item de content, que √© um objeto TextContent
                    resposta = assistant_messages_for_this_run[0].content[0].text.value

                    # Traduz a resposta
                    resposta_traduzida = traduzir_resposta(resposta)

                    # Exibe a resposta traduzida
                    st.markdown(f"**Resposta:**\n\n{resposta_traduzida}")

                    # Adicionar resposta do assistente ao hist√≥rico (opcional)
                    # st.session_state.messages.append({"role": "assistant", "content": resposta_traduzida})
                    # with st.chat_message("assistant"):
                    #    st.markdown(resposta_traduzida)

                else:
                     st.warning("Assistente n√£o gerou uma resposta para esta solicita√ß√£o nesta execu√ß√£o.")


            elif run.status == "requires_action":
                 st.warning("A execu√ß√£o requer a√ß√£o (ex: uso de fun√ß√£o externa n√£o implementada).")
                 # Se voc√™ tiver tools de "function calling", precisaria implementar a l√≥gica aqui.
                 # Para file_search, geralmente n√£o chega neste status a menos que haja um erro.

            else:
                st.error(f"Erro ao processar a solicita√ß√£o. Status do Run: {run.status}")
                # Opcional: Recuperar passos do run para mais detalhes do erro
                # run_steps = client.beta.threads.runs.steps.list(thread_id=thread.id, run_id=run.id)
                # st.json(run_steps.data)


        except Exception as e:
            st.error(f"Erro na consulta: {str(e)}")
            # Opcional: Em caso de erro grave na thread, pode ser √∫til iniciar uma nova na pr√≥xima vez
            # st.session_state.thread_id = None

# Nota: A exclus√£o do Vector Store e do Assistente ao final da sess√£o n√£o √© autom√°tica.
# Eles persistir√£o na sua conta da OpenAI at√© que voc√™ os exclua manualmente via API, CLI ou playground.
# Em um ambiente de produ√ß√£o, voc√™ provavelmente gerenciaria o ciclo de vida desses recursos
# de forma mais controlada.
